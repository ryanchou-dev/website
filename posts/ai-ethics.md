---
title: "ethical blame"
date: "2023-04-04"
abstract: "and thoughts about how we should train our beloved ai models."
tags: ["vents"]
---

![](https://images.unsplash.com/photo-1542384701-9eaf70a33558?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1740&q=80)
"_my hands will be their hands until i'm free_" - yukon (interlude)

---

Greetings! I'll try to write more in the coming days :D. Here is a messy vent essay that I wrote a couple of nights ago, don't take anything i say too seriously ;>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recently, I have been obsessively reading about ethics in artificial intelligence, and what it means for the continued automation of daily tasks. Naturally, people would like to think of themselves as utilitarian, that is, "logically" correct and assuming the best for the outcome. Yet, everybody seemingly cannot make the right decision. Why is this, and who is to blame if an inhuman entity hurts humans?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Edison once said, "Non-violence leads to the highest ethics, which is the goal of all evolution..." Upon first glance, this seems reasonable. After all, we all possess innate tendencies to not harm others. But, of course, in a life-or-death situation, this would often entail freezing up and avoiding action, to stray away from the problem. This is a flaw, isn't it? Thus, one would argue that we should train any intelligence model to be better than us, to supersede our mistakes. Harm others if it means that more people are safe. Though, it just feels wrong to build something with the power to hurt others. On paper, we are told that this would lead to a world with minimal loss, and ideally, we should chase this. This implementation seems so correct that I could just imagine a world where all autonomous units thought like this, but it's hopelessly naive. After all, we can never know anything with absolute certainty. It's almost like this problem wasn't meant to be solved; stepping away would always make us vulnerable and helpless, and being bold might hurt us and others around us. Even if there was some common agreed-upon stance to take, there is just so much data to analyze. If we're running into a car, where should we turn? What is the other driver thinking about doing? If we both slow down, we could try to minimize damage, but who knows what they'll do, etc. Simply put, the problem of ethics in AI needs more time to develop before we can safely deploy it.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another problem arises with blame and learning. There isn't a good feedback loop to learn what ethics are and the best decision to make in any case. The most minuscule factors could completely flip the desired output, and this makes us vulnerable to life-threatening mistakes. Who is to blame if something goes south? Is it the developers? What can they even do?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I'm not entirely sure what the path to fixing this is. Maybe the only right solution at the moment is to return to simplicity. That is **you** in **your** car, using **your** judgment to make the best decision for **you**.

~ cryan
